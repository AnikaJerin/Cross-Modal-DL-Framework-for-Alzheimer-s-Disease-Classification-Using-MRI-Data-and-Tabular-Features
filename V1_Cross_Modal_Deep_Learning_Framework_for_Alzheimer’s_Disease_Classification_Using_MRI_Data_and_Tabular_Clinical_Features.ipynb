{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ***Cell 1 — Imports and setup***"
      ],
      "metadata": {
        "id": "MARdbuBlGVOG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "id": "uTcK0f3__YEb",
        "outputId": "c61cd323-0863-480f-c2ee-c9864be98a9f"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-724000440.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# !rm -rf /content/drive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    132\u001b[0m   )\n\u001b[1;32m    133\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    135\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ],
      "source": [
        "# Cell 1: Mount Drive, imports, paths\n",
        "# !rm -rf /content/drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nibabel as nib\n",
        "import zipfile\n",
        "import tempfile\n",
        "import timm\n",
        "import torchvision.transforms as T\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "\n",
        "CSV_PATH = \"/content/drive/MyDrive/merged_ADNI_dataset.csv\"\n",
        "ZIP_PATH = \"/content/drive/MyDrive/ADNI_clean_data.zip\"\n",
        "OUT_DIR = \"/content/drive/MyDrive/ADNI_project_outputs\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 2 — Load CSV and **inspect** *italicised text*"
      ],
      "metadata": {
        "id": "yDvzhzJHGdF9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Load CSV\n",
        "if not os.path.exists(CSV_PATH):\n",
        "    raise FileNotFoundError(f\"CSV not found at {CSV_PATH}. Please check the path!\")\n",
        "\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "print(\"CSV rows:\", len(df))\n",
        "print(\"Columns:\", df.columns.tolist())\n",
        "print(\"Group counts:\\n\", df['Group'].value_counts())\n",
        "\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "kIvtgjSJGeuY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Cell 3 — List NIfTI files in ZIP***"
      ],
      "metadata": {
        "id": "QOK_0yltIHhm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: List NIfTI files in ZIP\n",
        "with zipfile.ZipFile(ZIP_PATH, 'r') as zf:\n",
        "    nii_files = [n for n in zf.namelist() if n.lower().endswith(('.nii', '.nii.gz'))]\n",
        "\n",
        "print(\"Total NIfTI files in ZIP:\", len(nii_files))\n",
        "print(\"First 10 files:\\n\", nii_files[:10])\n"
      ],
      "metadata": {
        "id": "f-0hfr-XIML-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Cell 4 — Read NIfTI directly from ZIP***"
      ],
      "metadata": {
        "id": "zp3tjwCbIRMN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Robust NIfTI reader\n",
        "import tempfile\n",
        "import zipfile\n",
        "import nibabel as nib\n",
        "import os\n",
        "\n",
        "# Filter valid files\n",
        "with zipfile.ZipFile(ZIP_PATH, 'r') as zf:\n",
        "    nii_files = [n for n in zf.namelist()\n",
        "                 if n.lower().endswith(('.nii', '.nii.gz')) and '__macosx' not in n.lower()]\n",
        "\n",
        "print(\"Total valid NIfTI files in ZIP:\", len(nii_files))\n",
        "print(\"First 10 files:\\n\", nii_files[:10])\n",
        "\n",
        "def read_nifti_from_zip(zip_path, nii_filename):\n",
        "    \"\"\"\n",
        "    Reads a NIfTI file (.nii or .nii.gz) directly from ZIP using a temporary file.\n",
        "    Keeps original extension to avoid ImageFileError.\n",
        "    \"\"\"\n",
        "    ext = os.path.splitext(nii_filename)[1]  # .nii or .gz\n",
        "    if ext.lower() == '.gz':  # double extension .nii.gz\n",
        "        suffix = '.nii.gz'\n",
        "    else:\n",
        "        suffix = '.nii'\n",
        "\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zf:\n",
        "        with zf.open(nii_filename) as f:\n",
        "            with tempfile.NamedTemporaryFile(suffix=suffix) as tmp:\n",
        "                tmp.write(f.read())\n",
        "                tmp.flush()\n",
        "                nim = nib.load(tmp.name)\n",
        "                vol = nim.get_fdata()\n",
        "    return vol\n",
        "\n",
        "# Quick test\n",
        "try:\n",
        "    vol = read_nifti_from_zip(ZIP_PATH, nii_files[0])\n",
        "    print(\"First volume shape:\", vol.shape)\n",
        "except Exception as e:\n",
        "    print(\"Failed to read first NIfTI:\", e)\n"
      ],
      "metadata": {
        "id": "J6tdfIqzISmB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Cell 5 — Convert 3D volume to 2D slices***"
      ],
      "metadata": {
        "id": "204cLK27IWJh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Convert 3D volume to slices\n",
        "def get_slices_from_volume(vol, n_slices=16, out_size=224):\n",
        "    \"\"\"\n",
        "    vol: 3D numpy array (Z,H,W)\n",
        "    n_slices: number of slices to sample\n",
        "    out_size: resize slices to (H,W)\n",
        "    \"\"\"\n",
        "    z = vol.shape[0]\n",
        "    idxs = np.linspace(0, z-1, n_slices).astype(int)\n",
        "    slices = []\n",
        "    for i in idxs:\n",
        "        sl = vol[i,:,:]\n",
        "        sl = resize(sl, (out_size, out_size), preserve_range=True, anti_aliasing=True)\n",
        "        sl = (sl - sl.mean()) / (sl.std() if sl.std() > 0 else 1.0)  # normalize\n",
        "        sl3 = np.stack([sl, sl, sl], axis=0)  # 3 channels\n",
        "        slices.append(sl3.astype(np.float32))\n",
        "    return np.stack(slices, axis=0)  # (n_slices, 3, H, W)\n"
      ],
      "metadata": {
        "id": "Q6mQx2CGIY_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Cell 6 — Dataset class (reads slices + tabular features)bold text***"
      ],
      "metadata": {
        "id": "0-Fk8l6GIcTs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Cell 6: Slice-based Dataset\n",
        "# # Cell 6: safer SliceDataset mapping by CSV filename\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "import nibabel as nib\n",
        "import zipfile\n",
        "import tempfile\n",
        "import cv2\n",
        "\n",
        "class SliceDataset(Dataset):\n",
        "    def __init__(self, df, zip_path, n_slices=16, tabular_cols=['Age','Sex','MMSCORE'], filename_col='nii_filename'):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.zip_path = zip_path\n",
        "        self.n_slices = n_slices\n",
        "        self.tabular_cols = tabular_cols\n",
        "        self.filename_col = filename_col\n",
        "\n",
        "        # Map groups to integer labels\n",
        "        self.label_map = {'CN': 0, 'MCI': 1, 'AD': 2}\n",
        "        self.df = self.df[self.df['Group'].isin(self.label_map.keys())].reset_index(drop=True)\n",
        "\n",
        "        # I just wanted to ensure valid numeric MMSE and Age\n",
        "        self.df['Age'] = pd.to_numeric(self.df['Age'], errors='coerce').fillna(self.df['Age'].median())\n",
        "        self.df['MMSCORE'] = pd.to_numeric(self.df['MMSCORE'], errors='coerce').fillna(self.df['MMSCORE'].median())\n",
        "        self.df['Sex'] = self.df['Sex'].map({'F': 0, 'M': 1}).fillna(0)\n",
        "\n",
        "        from collections import Counter\n",
        "        print(\"Label counts in dataset:\", Counter(self.df['Group']))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def read_nifti(self, nii_filename):\n",
        "        \"\"\"Read a NIfTI file directly from ZIP.\"\"\"\n",
        "        suffix = '.nii.gz' if nii_filename.endswith('.gz') else '.nii'\n",
        "        with zipfile.ZipFile(self.zip_path, 'r') as zf:\n",
        "            with zf.open(nii_filename) as f:\n",
        "                with tempfile.NamedTemporaryFile(suffix=suffix) as tmp:\n",
        "                    tmp.write(f.read())\n",
        "                    tmp.flush()\n",
        "                    img = nib.load(tmp.name).get_fdata()\n",
        "        return img\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        nii_path = row[self.filename_col]\n",
        "        vol = self.read_nifti(nii_path)\n",
        "\n",
        "        # Normalize to 0-1\n",
        "        vol = (vol - np.min(vol)) / (np.ptp(vol) + 1e-8)\n",
        "\n",
        "        # Take n_slices evenly spaced across the z-axis\n",
        "        z_slices = np.linspace(0, vol.shape[2] - 1, self.n_slices, dtype=int)\n",
        "        slices = np.stack([cv2.resize(vol[:, :, z], (224, 224)) for z in z_slices], axis=0)\n",
        "\n",
        "        # Convert to 3-channel (repeat)\n",
        "        slices = np.repeat(slices[..., None], 3, axis=-1)  # (n_slices, 224, 224, 3)\n",
        "        slices = torch.tensor(slices).permute(0, 3, 1, 2).float()  # (n_slices, 3, 224, 224)\n",
        "\n",
        "        # Tabular data\n",
        "        tab = torch.tensor([row[c] for c in self.tabular_cols], dtype=torch.float32)\n",
        "\n",
        "        label = torch.tensor(self.label_map[row['Group']], dtype=torch.long)\n",
        "        return {'slices': slices, 'tab': tab, 'label': label}\n",
        "\n"
      ],
      "metadata": {
        "id": "oGQqEuwlIeWE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Cell 7 — Model Definition (Slice CNN + Tabular Fusion)***"
      ],
      "metadata": {
        "id": "8y3maxzMIpuK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: Slice-based EfficientNet + Tabular Fusion\n",
        "class SliceEfficientNetFusion(nn.Module):\n",
        "    def __init__(self, backbone_name='efficientnet_b0', pretrained=True, n_slices=16, tabular_dim=3, fuse_dim=256, n_classes=3):\n",
        "        super().__init__()\n",
        "        self.n_slices = n_slices\n",
        "        # Pretrained 2D CNN backbone (output feature vector)\n",
        "        self.backbone = timm.create_model(backbone_name, pretrained=pretrained, num_classes=0, global_pool='avg')\n",
        "        feat_dim = self.backbone.num_features\n",
        "        self.img_fc = nn.Linear(feat_dim, fuse_dim)\n",
        "\n",
        "        # Tabular features MLP\n",
        "        self.tab_fc = nn.Sequential(nn.Linear(tabular_dim, fuse_dim//2), nn.ReLU())\n",
        "\n",
        "        # Fusion MLP\n",
        "        self.fuse = nn.Sequential(\n",
        "            nn.Linear(fuse_dim + fuse_dim//2, fuse_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3)\n",
        "        )\n",
        "\n",
        "        # Final classifier\n",
        "        self.classifier = nn.Linear(fuse_dim, n_classes)\n",
        "\n",
        "    def forward(self, slices, tab):\n",
        "        # slices: (B, n_slices, 3, H, W)\n",
        "        B = slices.shape[0]\n",
        "        x = slices.view(B * self.n_slices, 3, slices.shape[-2], slices.shape[-1])\n",
        "        feats = self.backbone(x)  # (B*n_slices, feat_dim)\n",
        "        feats = feats.view(B, self.n_slices, -1).mean(dim=1)  # average across slices\n",
        "        img_emb = self.img_fc(feats)\n",
        "        tab_emb = self.tab_fc(tab)\n",
        "        f = torch.cat([img_emb, tab_emb], dim=1)\n",
        "        f = self.fuse(f)\n",
        "        out = self.classifier(f)\n",
        "        return out\n",
        "\n",
        "# Instantiate model\n",
        "tabular_cols = ['Age','Sex','MMSCORE']  # adjust based on your CSV\n",
        "model = SliceEfficientNetFusion(tabular_dim=len(tabular_cols), n_slices=16, n_classes=3).to(device)\n",
        "print(model)\n"
      ],
      "metadata": {
        "id": "6QqlrSP8IsRA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Just new cell  -_- Added Later : Build filename mapping ---\n",
        "\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "with zipfile.ZipFile(ZIP_PATH, 'r') as zf:\n",
        "    nii_files = [\n",
        "        n for n in zf.namelist()\n",
        "        if n.lower().endswith(('.nii', '.nii.gz')) and '__macosx' not in n.lower()\n",
        "    ]\n",
        "\n",
        "print(f\"Clean NIfTI count: {len(nii_files)}\")\n",
        "\n",
        "# Extract ID (e.g. I73937) from filenames\n",
        "id_to_file = {}\n",
        "pattern = re.compile(r'I\\d+')\n",
        "\n",
        "for f in nii_files:\n",
        "    match = pattern.search(f)\n",
        "    if match:\n",
        "        id_to_file[match.group()] = f\n",
        "\n",
        "# Match CSV rows\n",
        "df['nii_filename'] = df['Image Data ID'].map(id_to_file)\n",
        "\n",
        "# Show mapping stats\n",
        "print(\"Rows with matched NIfTI:\", df['nii_filename'].notna().sum())\n",
        "print(\"Unmatched rows:\", df['nii_filename'].isna().sum())\n",
        "\n",
        "# Show examples\n",
        "print(\"\\nSample matched rows:\")\n",
        "print(df[['Image Data ID', 'nii_filename', 'Group']].head(10))\n"
      ],
      "metadata": {
        "id": "MYMGkjfFy0nx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- This one too... filters valid rows and verifies class balance. becuase I couldn't find better dataset ---\n",
        "\n",
        "# Keep only rows with valid NIfTI paths\n",
        "df_matched = df[df['nii_filename'].notna()].reset_index(drop=True)\n",
        "\n",
        "# Ensure group mapping is consistent\n",
        "df_matched = df_matched[df_matched['Group'].isin(['AD', 'CN', 'MCI'])]\n",
        "\n",
        "print(f\"Final dataset size: {len(df_matched)}\")\n",
        "print(\"Label distribution:\")\n",
        "print(df_matched['Group'].value_counts())\n",
        "\n",
        "# Verify a few random samples\n",
        "print(\"\\nRandom sample rows:\")\n",
        "print(df_matched.sample(5)[['Subject', 'Group', 'Age', 'Sex', 'MMSCORE', 'nii_filename']])\n"
      ],
      "metadata": {
        "id": "3Ny0w1PxtVz9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Cell 8 — Training and Evaluation (5-Fold CV, Metrics, Plots)***"
      ],
      "metadata": {
        "id": "on5-dFFaIyKT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8: Training + Evaluation with Class-Weighted Loss\n",
        "\n",
        "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
        "from torch.optim import AdamW\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "\n",
        "labels = df_matched['Group'].map({'CN':0,'MCI':1,'AD':2}).values\n",
        "\n",
        "# Class counts and weights\n",
        "unique, counts = np.unique(labels, return_counts=True)\n",
        "class_counts = {int(k): int(v) for k, v in zip(unique, counts)}\n",
        "print(\"Class counts (derived):\", class_counts)\n",
        "\n",
        "cw = np.zeros(3, dtype=np.float32)\n",
        "for c in range(3):\n",
        "    cw[c] = 1.0 / class_counts.get(c, 1)\n",
        "class_weights = torch.tensor(cw, dtype=torch.float32).to(device)\n",
        "print(\"Class weights used for criterion:\", class_weights.cpu().numpy())\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "# 5-fold Stratified CV.. I need to tune the, later\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "fold_results = []\n",
        "\n",
        "EPOCHS = 30\n",
        "BATCH_SIZE = 8\n",
        "LR = 1e-4\n",
        "patience = 6\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(skf.split(np.zeros(len(labels)), labels)):\n",
        "    print(f\"\\n=== Fold {fold+1} ===\")\n",
        "    train_df = df_matched.iloc[train_idx].reset_index(drop=True)\n",
        "    val_df = df_matched.iloc[val_idx].reset_index(drop=True)\n",
        "\n",
        "    # datasets\n",
        "    train_dataset = SliceDataset(train_df, ZIP_PATH, n_slices=16, tabular_cols=tabular_cols, filename_col='nii_filename')\n",
        "    val_dataset   = SliceDataset(val_df, ZIP_PATH, n_slices=16, tabular_cols=tabular_cols, filename_col='nii_filename')\n",
        "\n",
        "    # Weighted sampler for training\n",
        "    train_labels_arr = labels[train_idx]\n",
        "    sample_weights = np.array([1.0 / class_counts[int(l)] for l in train_labels_arr], dtype=np.float32)\n",
        "    sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=sampler, num_workers=2)  # reduce workers to avoid freeze\n",
        "    val_loader   = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "\n",
        "    # model & optimizer\n",
        "    model = SliceEfficientNetFusion(tabular_dim=len(tabular_cols), n_slices=16, n_classes=3).to(device)\n",
        "    optimizer = AdamW(model.parameters(), lr=LR, weight_decay=1e-4)\n",
        "\n",
        "    best_macro_f1 = -1.0\n",
        "    bad_epochs = 0\n",
        "\n",
        "    for epoch in range(1, EPOCHS+1):\n",
        "        # --- Train ---\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for batch in tqdm(train_loader, desc=f\"Fold{fold+1} Train E{epoch}\"):\n",
        "            slices = batch['slices'].to(device)\n",
        "            tab = batch['tab'].to(device)\n",
        "            labels_t = batch['label'].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(slices, tab)\n",
        "            loss = criterion(outputs, labels_t)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item() * labels_t.size(0)\n",
        "\n",
        "        avg_train_loss = running_loss / len(train_loader.dataset)\n",
        "\n",
        "        # --- Validation ---\n",
        "        model.eval()\n",
        "        all_preds, all_labels = [], []\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                slices = batch['slices'].to(device)\n",
        "                tab = batch['tab'].to(device)\n",
        "                labels_t = batch['label'].to(device)\n",
        "                outputs = model(slices, tab)\n",
        "                loss = criterion(outputs, labels_t)\n",
        "                val_loss += loss.item() * labels_t.size(0)\n",
        "                preds = torch.argmax(outputs, dim=1)\n",
        "                all_preds.extend(preds.cpu().numpy().tolist())\n",
        "                all_labels.extend(labels_t.cpu().numpy().tolist())\n",
        "\n",
        "        avg_val_loss = val_loss / len(val_loader.dataset)\n",
        "        val_macro_f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
        "        print(f\"Epoch {epoch}: TrainLoss={avg_train_loss:.4f}, ValLoss={avg_val_loss:.4f}, ValMacroF1={val_macro_f1:.4f}\")\n",
        "\n",
        "        # early stopping & checkpoint\n",
        "        if val_macro_f1 > best_macro_f1:\n",
        "            best_macro_f1 = val_macro_f1\n",
        "            bad_epochs = 0\n",
        "            torch.save(model.state_dict(), f\"best_model_fold{fold+1}.pt\")\n",
        "            torch.save(model.state_dict(), f\"{OUT_DIR}/best_model_fold{fold+1}.pt\")\n",
        "\n",
        "        else:\n",
        "            bad_epochs += 1\n",
        "            if bad_epochs >= patience:\n",
        "                print(f\"Early stopping: no improvement for {patience} epochs.\")\n",
        "                break\n",
        "\n",
        "    # --- Final evaluation ---\n",
        "    model.load_state_dict(torch.load(f\"best_model_fold{fold+1}.pt\"))\n",
        "    model.eval()\n",
        "    all_preds, all_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            slices = batch['slices'].to(device)\n",
        "            tab = batch['tab'].to(device)\n",
        "            labels_t = batch['label'].to(device)\n",
        "            outputs = model(slices, tab)\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            all_preds.extend(preds.cpu().numpy().tolist())\n",
        "            all_labels.extend(labels_t.cpu().numpy().tolist())\n",
        "\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    print(\"Fold Confusion Matrix:\\n\", cm)\n",
        "    print(\"Fold Classification Report:\\n\", classification_report(all_labels, all_preds, target_names=['CN','MCI','AD'], zero_division=0))\n",
        "\n",
        "    fold_results.append({'fold': fold+1, 'cm': cm, 'report': classification_report(all_labels, all_preds, target_names=['CN','MCI','AD'], output_dict=True, zero_division=0)})\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BV1z7KVDI1Og"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}